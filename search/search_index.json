{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction VOICEN is an open source project to build voice-enabled devices. The first goal of the project is to make an open source smart speaker. With an open source smart speaker, you have the freedom to choose which voice assistant to use.You have full control of your device to install apps and to share your data. Hardware The first hardware of the project is a DIY smart speaker kit - VOICEN Linear 4 Mic Array Kit . It will be available at the end of November. You can pre-order it from https://www.makerfabs.com/voicen-linear-4-mic-array-kit.html Applications Built a smart speaker with multiple voice assistants Make a Wi-Fi speaker with AirPlay and UPnP/DLNA Setup a multi-room audio system Automate your home with HomeAutomation Learn to use a variety of audio processing algorithms such as Acoustic Echo Cancellation (AEC), Direction Of + Arrival (DOA), Noise Suppression (NS) and Beamforming Learn how to send data over audio","title":"Introduction"},{"location":"#introduction","text":"VOICEN is an open source project to build voice-enabled devices. The first goal of the project is to make an open source smart speaker. With an open source smart speaker, you have the freedom to choose which voice assistant to use.You have full control of your device to install apps and to share your data.","title":"Introduction"},{"location":"#hardware","text":"The first hardware of the project is a DIY smart speaker kit - VOICEN Linear 4 Mic Array Kit . It will be available at the end of November. You can pre-order it from https://www.makerfabs.com/voicen-linear-4-mic-array-kit.html","title":"Hardware"},{"location":"#applications","text":"Built a smart speaker with multiple voice assistants Make a Wi-Fi speaker with AirPlay and UPnP/DLNA Setup a multi-room audio system Automate your home with HomeAutomation Learn to use a variety of audio processing algorithms such as Acoustic Echo Cancellation (AEC), Direction Of + Arrival (DOA), Noise Suppression (NS) and Beamforming Learn how to send data over audio","title":"Applications"},{"location":"airplay/","text":"Shairport-Sync for AirPlay Shairport Sync is an AirPlay audio player \u2013 it plays audio streamed from iTunes, iOS, Apple TV and macOS devices and AirPlay sources such as Quicktime Player and ForkedDaapd, among others. Audio played by a Shairport Sync-powered device stays synchronised with the source and hence with similar devices playing the same source. In this way, synchronised multi-room audio is possible for players that support it, such as iTunes. Install sudo apt install shairport-sync This will install shairport-sync and start the systemd service shairport-sync.service . Configure The configuration file is /etc/shairport-sync.conf . After you change the configuration, you should restart shairport-sync by running sudo systemctl restart shairport-sync Using AirPlay on iOS play any music and open iOS control center Choose the AirPlay device","title":"AirPlay"},{"location":"airplay/#shairport-sync-for-airplay","text":"Shairport Sync is an AirPlay audio player \u2013 it plays audio streamed from iTunes, iOS, Apple TV and macOS devices and AirPlay sources such as Quicktime Player and ForkedDaapd, among others. Audio played by a Shairport Sync-powered device stays synchronised with the source and hence with similar devices playing the same source. In this way, synchronised multi-room audio is possible for players that support it, such as iTunes.","title":"Shairport-Sync for AirPlay"},{"location":"airplay/#install","text":"sudo apt install shairport-sync This will install shairport-sync and start the systemd service shairport-sync.service .","title":"Install"},{"location":"airplay/#configure","text":"The configuration file is /etc/shairport-sync.conf . After you change the configuration, you should restart shairport-sync by running sudo systemctl restart shairport-sync","title":"Configure"},{"location":"airplay/#using-airplay-on-ios","text":"play any music and open iOS control center Choose the AirPlay device","title":"Using AirPlay on iOS"},{"location":"alexa/","text":"Alexa Voice Service Python avs C++ avs-device-sdk","title":"Alexa"},{"location":"alexa/#alexa-voice-service","text":"","title":"Alexa Voice Service"},{"location":"alexa/#python","text":"avs","title":"Python"},{"location":"alexa/#c","text":"avs-device-sdk","title":"C++"},{"location":"alsa/","text":"ALSA The Advanced Linux Sound Architecture (ALSA) provides audio and MIDI functionality to the Linux operating system. ALSA has the following significant features: Efficient support for all types of audio interfaces, from consumer sound cards to professional multichannel audio interfaces. Fully modularized sound drivers. Advanced Linux Sound Architecture (ALSA) is a software framework and part of the Linux kernel that provides an application programming interface (API) for sound card device drivers. ALSA devices run arecord -l to list capture devices or run aplay -l to list playback devices. Capture arecord -f S16_LE -c 4 -r 16000 -d 3 -v audio.wav","title":"ALSA"},{"location":"alsa/#alsa","text":"The Advanced Linux Sound Architecture (ALSA) provides audio and MIDI functionality to the Linux operating system. ALSA has the following significant features: Efficient support for all types of audio interfaces, from consumer sound cards to professional multichannel audio interfaces. Fully modularized sound drivers. Advanced Linux Sound Architecture (ALSA) is a software framework and part of the Linux kernel that provides an application programming interface (API) for sound card device drivers.","title":"ALSA"},{"location":"alsa/#alsa-devices","text":"run arecord -l to list capture devices or run aplay -l to list playback devices.","title":"ALSA devices"},{"location":"alsa/#capture","text":"arecord -f S16_LE -c 4 -r 16000 -d 3 -v audio.wav","title":"Capture"},{"location":"assembling/","text":"Assembling","title":"Assembling"},{"location":"assembling/#assembling","text":"","title":"Assembling"},{"location":"get_one/","text":"VOICEN Linear 4 Mic Array Kit You can get one from https://www.makerfabs.com/voicen-linear-4-mic-array-kit.html","title":"Get one"},{"location":"get_one/#voicen-linear-4-mic-array-kit","text":"You can get one from https://www.makerfabs.com/voicen-linear-4-mic-array-kit.html","title":"VOICEN Linear 4 Mic Array Kit"},{"location":"get_started/","text":"Get Started Prepare a Micro SD card Download a VOICEN OS image (.zip) from OneDrive from \u767e\u5ea6\u7f51\u76d8 Download and install Etcher use Etcher to write the latest VOICEN OS image into a Micro SD card Insert the micro SD card into Nanopi Neo Air and power on the device. It may take 40 seconds to start. Setup WiFi Press and hold the touch key for more than 4 seconds to turn the device into WiFi setup mode. Go to the web app - Hey WiFi to broadcast WiFi settings over sound. After the device received the WiFi settings and connected to the WiFi network, it sends its IP address to the web app, and then the web app generates a link which is linked to the device dashboard. Click the link with the device's IP address, we will get the dashboard of the device. Device Dashboard Add a New User Using Web Terminal We can open a web terminal to add a new user. The default user is root and its password is 1234 . After a new user is created, the user root will be no longer allowed to login the web terminal. Play with Jupyter Lab We can also open JupyterLab which is a very powerful web IDE. It allows you to create and share documents that contain live code, equations, visualizations and narrative text. In this document, you can run python code or shell commands to play sound, make a plot and analyze data.","title":"Get Started"},{"location":"get_started/#get-started","text":"","title":"Get Started"},{"location":"get_started/#prepare-a-micro-sd-card","text":"Download a VOICEN OS image (.zip) from OneDrive from \u767e\u5ea6\u7f51\u76d8 Download and install Etcher use Etcher to write the latest VOICEN OS image into a Micro SD card Insert the micro SD card into Nanopi Neo Air and power on the device. It may take 40 seconds to start.","title":"Prepare a Micro SD card"},{"location":"get_started/#setup-wifi","text":"Press and hold the touch key for more than 4 seconds to turn the device into WiFi setup mode. Go to the web app - Hey WiFi to broadcast WiFi settings over sound. After the device received the WiFi settings and connected to the WiFi network, it sends its IP address to the web app, and then the web app generates a link which is linked to the device dashboard. Click the link with the device's IP address, we will get the dashboard of the device.","title":"Setup WiFi"},{"location":"get_started/#device-dashboard","text":"","title":"Device Dashboard"},{"location":"get_started/#add-a-new-user-using-web-terminal","text":"We can open a web terminal to add a new user. The default user is root and its password is 1234 . After a new user is created, the user root will be no longer allowed to login the web terminal.","title":"Add a New User Using Web Terminal"},{"location":"get_started/#play-with-jupyter-lab","text":"We can also open JupyterLab which is a very powerful web IDE. It allows you to create and share documents that contain live code, equations, visualizations and narrative text. In this document, you can run python code or shell commands to play sound, make a plot and analyze data.","title":"Play with Jupyter Lab"},{"location":"google_assistant/","text":"Google Assistant https://developers.google.com/assistant/sdk/","title":"Google Assistant"},{"location":"google_assistant/#google-assistant","text":"https://developers.google.com/assistant/sdk/","title":"Google Assistant"},{"location":"hardware/","text":"Hardware VOICEN Linear 4 Mic Array Kit The kit lets you build a smart speaker from scratch. It can run a variety of voice assistants such as Amazon Alexa, Google Assistant, MyCroft, Snips Offline Voice Assistant and Baidu DuerOS. It supports AirPlay and UPnP/DLNA. You can turn it into a music box with Mopidy and MPD, or use it to automate your home with HomeAssistant. Specifications AllWinner H3 (Quad-Core ARM Cortex A7 @ 1.296 GHz) 512MB RAM 8GB eMMC Flash 16GB Micro SD Card 4 Channels ADC AC108 4 MEMS microphones (64dB SNR, -38dB Sensitivity) 4\u03a9 3W Loud Speaker 4 LEDs (RGBY) 1 Touch Key 1 Paper Case 1 Micro USB Cable","title":"Hardware"},{"location":"hardware/#hardware","text":"","title":"Hardware"},{"location":"hardware/#voicen-linear-4-mic-array-kit","text":"The kit lets you build a smart speaker from scratch. It can run a variety of voice assistants such as Amazon Alexa, Google Assistant, MyCroft, Snips Offline Voice Assistant and Baidu DuerOS. It supports AirPlay and UPnP/DLNA. You can turn it into a music box with Mopidy and MPD, or use it to automate your home with HomeAssistant.","title":"VOICEN Linear 4 Mic Array Kit"},{"location":"hardware/#specifications","text":"AllWinner H3 (Quad-Core ARM Cortex A7 @ 1.296 GHz) 512MB RAM 8GB eMMC Flash 16GB Micro SD Card 4 Channels ADC AC108 4 MEMS microphones (64dB SNR, -38dB Sensitivity) 4\u03a9 3W Loud Speaker 4 LEDs (RGBY) 1 Touch Key 1 Paper Case 1 Micro USB Cable","title":"Specifications"},{"location":"home_assistant/","text":"Home Assistant Home Assistant is a home automation platform running on Python 3. It is able to track and control all devices at home and offer a platform for automating control. Install sudo apt install libffi-dev python3-dev sudo python3 -m pip install homeassistant hass --open-ui Integrate devices Home Assistant supports a large mount of devices. You can a long list at https://www.home-assistant.io/integrations/","title":"Home Assistant"},{"location":"home_assistant/#home-assistant","text":"Home Assistant is a home automation platform running on Python 3. It is able to track and control all devices at home and offer a platform for automating control.","title":"Home Assistant"},{"location":"home_assistant/#install","text":"sudo apt install libffi-dev python3-dev sudo python3 -m pip install homeassistant hass --open-ui","title":"Install"},{"location":"home_assistant/#integrate-devices","text":"Home Assistant supports a large mount of devices. You can a long list at https://www.home-assistant.io/integrations/","title":"Integrate devices"},{"location":"io/","text":"IO Used name number usage PA2 2 AMP PWR_EN PA3 3 AMP_MUTE PC0 64 BLUE LED PC1 65 GREEN LED PC2 66 YELLOW LED PC3 67 RED LED PG11 203 TOUCH KEY To convert IO name to IO number: # n(PC3) = ( C - A ) * 32 + 3 = 2 * 32 + 3 = 67 name = PC3 number = ( ord ( name [ 1 ]) - ord ( A )) * 32 + int ( name [ 2 ]) There are several ways to control IO. Via IO registers (direct memory access) sudo apt install -y sunxi-tools sudo sunxi-pio -m PC1 = 1 # turn on green LED Accessing IO registers has the high priority. Via IO service By default, there is a IO service to detect the touch key and to control the LEDs and amplifier. The IO service provides MQTT message bus to access. mosquitto_pub -t /voicen/leds/value -m 0xf mosquitto_pub -t /voicen/leds/value -m 0x0 Via GPIO sysfs interface sudo su cd /sys/class/gpio echo 65 export # export the IO to userspace echo out gpio65/direction # set IO as output echo 1 gpio65/value echo 0 gpio65/value echo 65 unexport # Reverses the effect of exporting to userspace Via gpiod sudo apt install -y gpiod sudo gpioset 0 65 = 0 # turn on green LED sudo gpiomon 0 203 # detect touch event Reference https://github.com/brgl/libgpiod https://github.com/linux-sunxi/sunxi-tools https://www.kernel.org/doc/Documentation/gpio/sysfs.txt","title":"IO"},{"location":"io/#io","text":"","title":"IO"},{"location":"io/#used","text":"name number usage PA2 2 AMP PWR_EN PA3 3 AMP_MUTE PC0 64 BLUE LED PC1 65 GREEN LED PC2 66 YELLOW LED PC3 67 RED LED PG11 203 TOUCH KEY To convert IO name to IO number: # n(PC3) = ( C - A ) * 32 + 3 = 2 * 32 + 3 = 67 name = PC3 number = ( ord ( name [ 1 ]) - ord ( A )) * 32 + int ( name [ 2 ]) There are several ways to control IO.","title":"Used"},{"location":"io/#via-io-registers-direct-memory-access","text":"sudo apt install -y sunxi-tools sudo sunxi-pio -m PC1 = 1 # turn on green LED Accessing IO registers has the high priority.","title":"Via IO registers (direct memory access)"},{"location":"io/#via-io-service","text":"By default, there is a IO service to detect the touch key and to control the LEDs and amplifier. The IO service provides MQTT message bus to access. mosquitto_pub -t /voicen/leds/value -m 0xf mosquitto_pub -t /voicen/leds/value -m 0x0","title":"Via IO service"},{"location":"io/#via-gpio-sysfs-interface","text":"sudo su cd /sys/class/gpio echo 65 export # export the IO to userspace echo out gpio65/direction # set IO as output echo 1 gpio65/value echo 0 gpio65/value echo 65 unexport # Reverses the effect of exporting to userspace","title":"Via GPIO sysfs interface"},{"location":"io/#via-gpiod","text":"sudo apt install -y gpiod sudo gpioset 0 65 = 0 # turn on green LED sudo gpiomon 0 203 # detect touch event","title":"Via gpiod"},{"location":"io/#reference","text":"https://github.com/brgl/libgpiod https://github.com/linux-sunxi/sunxi-tools https://www.kernel.org/doc/Documentation/gpio/sysfs.txt","title":"Reference"},{"location":"jack/","text":"JACK Audio Connection Kit JACK Audio Connection Kit (or JACK; a recursive acronym) is a professional sound server daemon that provides real-time, low-latency connections for both audio and MIDI data between applications that use its API. There are two JACK implementations (JACK1 and JACK2), see this comparison for the difference between the two . In short, JACK1 and JACK 2 are equivalent implementations of the same protocol. JACK2 uses dbus to talk with other applications such as PulseAudio, X. Using JACK2 without X requires some additional hacks. So we use JACK1 instead of JACK2 here. Install sudo apt install jackd1 Use JACK as an ALSA PCM Edit /etc/asound.conf (system wide settings) to have these lines: pcm.rawjack { type jack playback_ports { 0 system:playback_1 } } pcm.jack { type plug slave { pcm rawjack } hint { description JACK Audio Connection Kit } } Run jackd -v -d alsa -d hw:Codec -P Play audio aplay -v -D jack audio.wav Configure $ cat /etc/security/limits.d/audio.conf # Provided by the jackd package. # # Changes to this file will be preserved. # # If you want to enable/disable realtime permissions, run # # dpkg-reconfigure -p high jackd @audio - rtprio 95 @audio - memlock unlimited #@audio - nice -19 Multiple users To setup JACK for multiple users, we can run JACK in promiscuous mode. start JACK with a systemd service echo [Unit] Description=Start the JACK server in promiscuous mode After=sound.target [Service] Type=simple Environment= JACK_PROMISCUOUS_SERVER= UMask=0 ExecStart=/usr/bin/jackd -d alsa -d hw:Codec -P -o 1 [Install] WantedBy=multi-user.target | sudo tee /etc/systemd/system/jackd.service sudo systemctl start jackd sudo systemctl enable jackd export the environment variable JACK_PROMISCUOUS_SERVER to let a JACK client use JACK JACK_PROMISCUOUS_SERVER= export JACK_PROMISCUOUS_SERVER aplay -v audio.wav Reference https://wiki.archlinux.org/index.php/JACK_Audio_Connection_Kit http://jack-audio.10948.n7.nabble.com/Jack-Devel-How-to-use-jackd-as-a-system-wide-server-td20053.html","title":"JACK"},{"location":"jack/#jack-audio-connection-kit","text":"JACK Audio Connection Kit (or JACK; a recursive acronym) is a professional sound server daemon that provides real-time, low-latency connections for both audio and MIDI data between applications that use its API. There are two JACK implementations (JACK1 and JACK2), see this comparison for the difference between the two . In short, JACK1 and JACK 2 are equivalent implementations of the same protocol. JACK2 uses dbus to talk with other applications such as PulseAudio, X. Using JACK2 without X requires some additional hacks. So we use JACK1 instead of JACK2 here.","title":"JACK Audio Connection Kit"},{"location":"jack/#install","text":"sudo apt install jackd1","title":"Install"},{"location":"jack/#use-jack-as-an-alsa-pcm","text":"Edit /etc/asound.conf (system wide settings) to have these lines: pcm.rawjack { type jack playback_ports { 0 system:playback_1 } } pcm.jack { type plug slave { pcm rawjack } hint { description JACK Audio Connection Kit } }","title":"Use JACK as an ALSA PCM"},{"location":"jack/#run","text":"jackd -v -d alsa -d hw:Codec -P","title":"Run"},{"location":"jack/#play-audio","text":"aplay -v -D jack audio.wav","title":"Play audio"},{"location":"jack/#configure","text":"$ cat /etc/security/limits.d/audio.conf # Provided by the jackd package. # # Changes to this file will be preserved. # # If you want to enable/disable realtime permissions, run # # dpkg-reconfigure -p high jackd @audio - rtprio 95 @audio - memlock unlimited #@audio - nice -19","title":"Configure"},{"location":"jack/#multiple-users","text":"To setup JACK for multiple users, we can run JACK in promiscuous mode. start JACK with a systemd service echo [Unit] Description=Start the JACK server in promiscuous mode After=sound.target [Service] Type=simple Environment= JACK_PROMISCUOUS_SERVER= UMask=0 ExecStart=/usr/bin/jackd -d alsa -d hw:Codec -P -o 1 [Install] WantedBy=multi-user.target | sudo tee /etc/systemd/system/jackd.service sudo systemctl start jackd sudo systemctl enable jackd export the environment variable JACK_PROMISCUOUS_SERVER to let a JACK client use JACK JACK_PROMISCUOUS_SERVER= export JACK_PROMISCUOUS_SERVER aplay -v audio.wav","title":"Multiple users"},{"location":"jack/#reference","text":"https://wiki.archlinux.org/index.php/JACK_Audio_Connection_Kit http://jack-audio.10948.n7.nabble.com/Jack-Devel-How-to-use-jackd-as-a-system-wide-server-td20053.html","title":"Reference"},{"location":"kws/","text":"KWS Keyword Spotting (KWS) or Keyword Search (alsa called Keyword Detection, Wake Word Detection) is a key technology to implement a hands-free voice assistant. It's used to detect words such as \"OK, Google\", \"Hey, Siri\" to start a conversation. Unlike speech recognition, KWS is a always-listening program which should be run locally without and offline. There are a variety of KWS projects, such as Snowboy , Mycroft Precise , Porcupine . Snowboy Install Snowboy sudo apt install -y libatlas-base-dev swig python3-pyaudio sox sudo pip3 install voice-engine git clone https://github.com/Kitt-AI/snowboy.git --depth = 1 cd snowboy/swig/Python3 make cd ../.. python3 setup.py build sudo python3 setup.py bdist_wheel sudo pip3 install --no-deps dist/snowboy-*-py3-none-any.whl Detect a keyword echo import time from voice_engine.source import Source from voice_engine.kws import KWS src = Source(rate=16000, channels=1, device_name= default ) kws = KWS(model= computer , sensitivity=0.6, verbose=True) src.pipeline(kws) def on_detected(keyword): print( \\ndetected {} .format(keyword)) kws.set_callback(on_detected) src.pipeline_start() while True: try: time.sleep(1) except KeyboardInterrupt: break src.pipeline_stop() kws.py python3 kws.py","title":"KWS"},{"location":"kws/#kws","text":"Keyword Spotting (KWS) or Keyword Search (alsa called Keyword Detection, Wake Word Detection) is a key technology to implement a hands-free voice assistant. It's used to detect words such as \"OK, Google\", \"Hey, Siri\" to start a conversation. Unlike speech recognition, KWS is a always-listening program which should be run locally without and offline. There are a variety of KWS projects, such as Snowboy , Mycroft Precise , Porcupine .","title":"KWS"},{"location":"kws/#snowboy","text":"Install Snowboy sudo apt install -y libatlas-base-dev swig python3-pyaudio sox sudo pip3 install voice-engine git clone https://github.com/Kitt-AI/snowboy.git --depth = 1 cd snowboy/swig/Python3 make cd ../.. python3 setup.py build sudo python3 setup.py bdist_wheel sudo pip3 install --no-deps dist/snowboy-*-py3-none-any.whl Detect a keyword echo import time from voice_engine.source import Source from voice_engine.kws import KWS src = Source(rate=16000, channels=1, device_name= default ) kws = KWS(model= computer , sensitivity=0.6, verbose=True) src.pipeline(kws) def on_detected(keyword): print( \\ndetected {} .format(keyword)) kws.set_callback(on_detected) src.pipeline_start() while True: try: time.sleep(1) except KeyboardInterrupt: break src.pipeline_stop() kws.py python3 kws.py","title":"Snowboy"},{"location":"message_bus/","text":"Message Bus The VOICEN OS uses MQTT as a message bus. We can control LEDs and AMP via the message bus. We can also subscribe touch key events from the message bus. Turn On/Off the LEDs mosquitto_pub -t /voicen/leds/value -m 0xf mosquitto_pub -t /voicen/leds/value -m 0x0 Enable or Disable the AMP mosquitto_pub -t /voicen/amp -m 1 mosquitto_pub -t /voicen/amp -m 0 Detect touch key events mosquitto_sub -t /voicen/touch How it works mosquitto is used as the MQTT broker. A IO service is running at background to receive IO control messages and send touch key event. You can enable, disable, start, stop or restart the IO service with systemctl : sudo systemctl stop io sudo systemctl start io sudo systemctl restart io sudo systemctl status io sudo systemctl disable io sudo systemctl enable io the IO program is a python script at /usr/local/bin/io_service.py the IO systemd service is at /etc/systemd/system/io.service","title":"Message Bus"},{"location":"message_bus/#message-bus","text":"The VOICEN OS uses MQTT as a message bus. We can control LEDs and AMP via the message bus. We can also subscribe touch key events from the message bus.","title":"Message Bus"},{"location":"message_bus/#turn-onoff-the-leds","text":"mosquitto_pub -t /voicen/leds/value -m 0xf mosquitto_pub -t /voicen/leds/value -m 0x0","title":"Turn On/Off the LEDs"},{"location":"message_bus/#enable-or-disable-the-amp","text":"mosquitto_pub -t /voicen/amp -m 1 mosquitto_pub -t /voicen/amp -m 0","title":"Enable or Disable the AMP"},{"location":"message_bus/#detect-touch-key-events","text":"mosquitto_sub -t /voicen/touch","title":"Detect touch key events"},{"location":"message_bus/#how-it-works","text":"mosquitto is used as the MQTT broker. A IO service is running at background to receive IO control messages and send touch key event. You can enable, disable, start, stop or restart the IO service with systemctl : sudo systemctl stop io sudo systemctl start io sudo systemctl restart io sudo systemctl status io sudo systemctl disable io sudo systemctl enable io the IO program is a python script at /usr/local/bin/io_service.py the IO systemd service is at /etc/systemd/system/io.service","title":"How it works"},{"location":"mopidy/","text":"Mopidy Mopidy is an extensible music server written in Python. Mopidy plays music from local disk, Spotify, SoundCloud, Google Play Music, and more. You edit the playlist from any phone, tablet, or computer using a range of MPD and web clients. Install sudo apt-get install build-essential python-dev python-pip \\ python-setuptools sudo apt-get install python-gst-1.0 \\ gir1.2-gstreamer-1.0 gir1.2-gst-plugins-base-1.0 \\ gstreamer1.0-plugins-good gstreamer1.0-plugins-ugly \\ gstreamer1.0-tools sudo pip2 install mopidy mopidy-musicbox-webclient These commands will install mopidy, its dependencies and mopidy webclient musicbox. Configure By default, Mopidy only listens only on the IPv4 loopback interface. To make it listen on all interfaces (both IPv4 and IPv6), create a configuration file /etc/mopidy/mopidy.conf . sudo mkdir /etc/mopidy echo [http] hostname = :: port = 6680 | sudo tee /etc/mopidy/mopidy.conf Run mopidy --config /etc/mopidy/mopidy.conf Music https://upload.wikimedia.org/wikipedia/en/c/c3/Super_Mario_Bros._theme.ogg Run as a service To make Mopidy autorun, we can create a systemd service. echo [Unit] Description=Mopidy music server After=avahi-daemon.service After=dbus.service After=network.target After=sound.target [Service] User=$USER ExecStart=/usr/local/bin/mopidy --config /etc/mopidy/mopidy.conf [Install] WantedBy=multi-user.target | sudo tee /etc/systemd/system/mopidy.service sudo systemctl start mopidy sudo systemctl enable mopidy For more details, please read the official document of Mopdiy","title":"Mopidy"},{"location":"mopidy/#mopidy","text":"Mopidy is an extensible music server written in Python. Mopidy plays music from local disk, Spotify, SoundCloud, Google Play Music, and more. You edit the playlist from any phone, tablet, or computer using a range of MPD and web clients.","title":"Mopidy"},{"location":"mopidy/#install","text":"sudo apt-get install build-essential python-dev python-pip \\ python-setuptools sudo apt-get install python-gst-1.0 \\ gir1.2-gstreamer-1.0 gir1.2-gst-plugins-base-1.0 \\ gstreamer1.0-plugins-good gstreamer1.0-plugins-ugly \\ gstreamer1.0-tools sudo pip2 install mopidy mopidy-musicbox-webclient These commands will install mopidy, its dependencies and mopidy webclient musicbox.","title":"Install"},{"location":"mopidy/#configure","text":"By default, Mopidy only listens only on the IPv4 loopback interface. To make it listen on all interfaces (both IPv4 and IPv6), create a configuration file /etc/mopidy/mopidy.conf . sudo mkdir /etc/mopidy echo [http] hostname = :: port = 6680 | sudo tee /etc/mopidy/mopidy.conf","title":"Configure"},{"location":"mopidy/#run","text":"mopidy --config /etc/mopidy/mopidy.conf","title":"Run"},{"location":"mopidy/#music","text":"https://upload.wikimedia.org/wikipedia/en/c/c3/Super_Mario_Bros._theme.ogg","title":"Music"},{"location":"mopidy/#run-as-a-service","text":"To make Mopidy autorun, we can create a systemd service. echo [Unit] Description=Mopidy music server After=avahi-daemon.service After=dbus.service After=network.target After=sound.target [Service] User=$USER ExecStart=/usr/local/bin/mopidy --config /etc/mopidy/mopidy.conf [Install] WantedBy=multi-user.target | sudo tee /etc/systemd/system/mopidy.service sudo systemctl start mopidy sudo systemctl enable mopidy For more details, please read the official document of Mopdiy","title":"Run as a service"},{"location":"music/","text":"Music Mopidy If you want to build a classic music box, you should try Mopidy . AirPlay When you are using an iOS device, you can cast your music through AirPlay UPnP/DLNA With an Android phone, you may want to stream your music through UPnP/DLNA","title":"Music"},{"location":"music/#music","text":"","title":"Music"},{"location":"music/#mopidy","text":"If you want to build a classic music box, you should try Mopidy .","title":"Mopidy"},{"location":"music/#airplay","text":"When you are using an iOS device, you can cast your music through AirPlay","title":"AirPlay"},{"location":"music/#upnpdlna","text":"With an Android phone, you may want to stream your music through UPnP/DLNA","title":"UPnP/DLNA"},{"location":"mycroft/","text":"MyCroft Mycroft is a hackable open source voice assistant. Install cd ~/ git clone https://github.com/MycroftAI/mycroft-core.git --depth=2 cd mycroft-core bash dev_setup.sh dev_setup.sh will install its dependencies and create a python virtual environment with virtualenv . Run cd ~/mycroft-core ./start-mycroft.sh debug https://github.com/MycroftAI/mycroft-core","title":"MyCroft"},{"location":"mycroft/#mycroft","text":"Mycroft is a hackable open source voice assistant.","title":"MyCroft"},{"location":"mycroft/#install","text":"cd ~/ git clone https://github.com/MycroftAI/mycroft-core.git --depth=2 cd mycroft-core bash dev_setup.sh dev_setup.sh will install its dependencies and create a python virtual environment with virtualenv .","title":"Install"},{"location":"mycroft/#run","text":"cd ~/mycroft-core ./start-mycroft.sh debug https://github.com/MycroftAI/mycroft-core","title":"Run"},{"location":"snapcast/","text":"Snapcast is a multi-room client-server audio player, where all clients are time synchronized with the server to play perfectly synced audio. It's not a standalone player, but an extension that turns your existing audio player into a Sonos-like multi-room solution. The server's audio input is a named pipe /tmp/snapfifo. All data that is fed into this file will be send to the connected clients. One of the most generic ways to use Snapcast is in conjunction with the music player daemon (MPD) or Mopidy, which can be configured to use a named pipe as audio output. https://github.com/badaix/snapcast","title":"Snapcast"},{"location":"snips/","text":"Snips Offline Voice Assistant Snips is an AI voice platform for connected devices that animates product interactions with customizable voice experiences. With Snips, we can create a Private by Design voice assistant that runs offline on the edge. Install sudo apt-get install -y dirmngr sudo bash -c echo deb https://raspbian.snips.ai/stretch stable main /etc/apt/sources.list.d/snips.list sudo apt-key adv --keyserver pgp.mit.edu --recv-keys D4F50CDCA10A2849 # or sudo apt-key adv --keyserver pgp.surfnet.nl --recv-keys D4F50CDCA10A2849 sudo apt-get update sudo apt-get install -y libgfortran3 sudo apt-get install -y snips-platform-voice snips-asr snips-dialogue snips-injection snips-tts snips-audio-server snips-hotword snips-nlu snips-watch sudo apt-get install snips-platform-demo","title":"Snips Voice Assistant"},{"location":"snips/#snips-offline-voice-assistant","text":"Snips is an AI voice platform for connected devices that animates product interactions with customizable voice experiences. With Snips, we can create a Private by Design voice assistant that runs offline on the edge.","title":"Snips Offline Voice Assistant"},{"location":"snips/#install","text":"sudo apt-get install -y dirmngr sudo bash -c echo deb https://raspbian.snips.ai/stretch stable main /etc/apt/sources.list.d/snips.list sudo apt-key adv --keyserver pgp.mit.edu --recv-keys D4F50CDCA10A2849 # or sudo apt-key adv --keyserver pgp.surfnet.nl --recv-keys D4F50CDCA10A2849 sudo apt-get update sudo apt-get install -y libgfortran3 sudo apt-get install -y snips-platform-voice snips-asr snips-dialogue snips-injection snips-tts snips-audio-server snips-hotword snips-nlu snips-watch sudo apt-get install snips-platform-demo","title":"Install"},{"location":"upnp/","text":"UPnP/DLNA DLNA uses Universal Plug and Play (UPnP) for media management, discovery and control. UPnP defines the types of device that DLNA supports (\"server\", \"renderer\", \"controller\") and the mechanisms for accessing media over a network. Here We use Mopidy + upmpdcli as a UPnP media render and a phone as a UPnP media controller to play music. Install Mopidy Go to Mopidy to install it. Install upmpdcli sudo apt-key adv --keyserver pool.sks-keyservers.net --recv-key 7808CE96D38B9201 echo deb http://www.lesbonscomptes.com/upmpdcli/downloads/raspbian/ stretch main | \\ sudo tee /etc/apt/sources.list.d/upmpdcli.list sudo apt update sudo apt install upmpdcli Configure The configuration file is /etc/upmpdcli.conf . You can rename the UPnP media render by changing friendlyname in /etc/upmpdcli.conf . Stream music","title":"UPnP/DLNA"},{"location":"upnp/#upnpdlna","text":"DLNA uses Universal Plug and Play (UPnP) for media management, discovery and control. UPnP defines the types of device that DLNA supports (\"server\", \"renderer\", \"controller\") and the mechanisms for accessing media over a network. Here We use Mopidy + upmpdcli as a UPnP media render and a phone as a UPnP media controller to play music.","title":"UPnP/DLNA"},{"location":"upnp/#install-mopidy","text":"Go to Mopidy to install it.","title":"Install Mopidy"},{"location":"upnp/#install-upmpdcli","text":"sudo apt-key adv --keyserver pool.sks-keyservers.net --recv-key 7808CE96D38B9201 echo deb http://www.lesbonscomptes.com/upmpdcli/downloads/raspbian/ stretch main | \\ sudo tee /etc/apt/sources.list.d/upmpdcli.list sudo apt update sudo apt install upmpdcli","title":"Install upmpdcli"},{"location":"upnp/#configure","text":"The configuration file is /etc/upmpdcli.conf . You can rename the UPnP media render by changing friendlyname in /etc/upmpdcli.conf .","title":"Configure"},{"location":"upnp/#stream-music","text":"","title":"Stream music"},{"location":"usb_sound_card/","text":"As a USB sound card By the default, the micro USB port of the Nanopi Neo Air is used as a serial port. While it can also be used a USB sound card. rm g_serial from the file /etc/module and reboot. run the script to setup a USB Audio Class 1.0 (UAC1) device #!/bin/bash modprobe libcomposite mkdir -p /sys/kernel/config/usb_gadget/g echo 0x1d6b /sys/kernel/config/usb_gadget/g/idVendor # Linux Foundation echo 0x0104 /sys/kernel/config/usb_gadget/g/idProduct # Multifunction Composite Gadget echo 0x0100 /sys/kernel/config/usb_gadget/g/bcdDevice # v1.0.0 echo 0x0200 /sys/kernel/config/usb_gadget/g/bcdUSB # USB 2.0 echo 0xef /sys/kernel/config/usb_gadget/g/bDeviceClass # USB 2.0 echo 0x02 /sys/kernel/config/usb_gadget/g/bDeviceSubClass # USB 2.0 echo 0x01 /sys/kernel/config/usb_gadget/g/bDeviceProtocol # USB 2.0 mkdir -p /sys/kernel/config/usb_gadget/g/strings/0x409 echo 000001 /sys/kernel/config/usb_gadget/g/strings/0x409/serialnumber echo VOICEN /sys/kernel/config/usb_gadget/g/strings/0x409/manufacturer echo devkit /sys/kernel/config/usb_gadget/g/strings/0x409/product mkdir -p /sys/kernel/config/usb_gadget/g/functions/uac1.usb0 # audio echo 0x1 /sys/kernel/config/usb_gadget/g/functions/uac1.usb0/c_chmask echo 48000 /sys/kernel/config/usb_gadget/g/functions/uac1.usb0/c_srate echo 0xf /sys/kernel/config/usb_gadget/g/functions/uac1.usb0/p_chmask echo 16000 /sys/kernel/config/usb_gadget/g/functions/uac1.usb0/p_srate mkdir -p /sys/kernel/config/usb_gadget/g/configs/c.1 echo 250 /sys/kernel/config/usb_gadget/g/configs/c.1/MaxPower ln -s /sys/kernel/config/usb_gadget/g/functions/uac1.usb0 /sys/kernel/config/usb_gadget/g/configs/c.1/ udevadm settle -t 5 || : ls /sys/class/udc/ /sys/kernel/config/usb_gadget/g/UDC run aplay -l and areocrd -l to check the virtual capture/playback devices created by the script $ aplay -l **** List of PLAYBACK Hardware Devices **** card 0: Codec [H3 Audio Codec], device 0: CDC PCM Codec-0 [] Subdevices: 1/1 Subdevice #0: subdevice #0 card 2: UAC1Gadget [UAC1_Gadget], device 0: UAC1_PCM [UAC1_PCM] Subdevices: 1/1 Subdevice #0: subdevice #0 $ arecord -l **** List of CAPTURE Hardware Devices **** card 0: Codec [H3 Audio Codec], device 0: CDC PCM Codec-0 [] Subdevices: 1/1 Subdevice #0: subdevice #0 card 1: voicen4mic [voicen4mic], device 0: 1c22000.i2s-ac108-codec0 ac108-codec.0-003b-0 [] Subdevices: 1/1 Subdevice #0: subdevice #0 card 2: UAC1Gadget [UAC1_Gadget], device 0: UAC1_PCM [UAC1_PCM] Subdevices: 1/1 Subdevice #0: subdevice #0 when playing an audio through the USB sound card on a computer, the audio is sent to the capture device hw:UAC1Gadget , and then run arecord -f S16_LE -c 1 -r 48000 -D hw:UAC1Gadget | aplay to play the audio on the device. when recording through the sound card on the computer, data is sent from the playback device hw:UAC1Gadget to the computer. a. use Audacity to record audio on a computer. To reocrd 4 (more than 2) channels audio, Windows WASAPI is selected b. run arecord -f S16_LE -c 4 -r 16000 | aplay -D hw:UAC1Gadget to send microphone data to the computer.","title":"As a USB Sound Card"},{"location":"usb_sound_card/#as-a-usb-sound-card","text":"By the default, the micro USB port of the Nanopi Neo Air is used as a serial port. While it can also be used a USB sound card. rm g_serial from the file /etc/module and reboot. run the script to setup a USB Audio Class 1.0 (UAC1) device #!/bin/bash modprobe libcomposite mkdir -p /sys/kernel/config/usb_gadget/g echo 0x1d6b /sys/kernel/config/usb_gadget/g/idVendor # Linux Foundation echo 0x0104 /sys/kernel/config/usb_gadget/g/idProduct # Multifunction Composite Gadget echo 0x0100 /sys/kernel/config/usb_gadget/g/bcdDevice # v1.0.0 echo 0x0200 /sys/kernel/config/usb_gadget/g/bcdUSB # USB 2.0 echo 0xef /sys/kernel/config/usb_gadget/g/bDeviceClass # USB 2.0 echo 0x02 /sys/kernel/config/usb_gadget/g/bDeviceSubClass # USB 2.0 echo 0x01 /sys/kernel/config/usb_gadget/g/bDeviceProtocol # USB 2.0 mkdir -p /sys/kernel/config/usb_gadget/g/strings/0x409 echo 000001 /sys/kernel/config/usb_gadget/g/strings/0x409/serialnumber echo VOICEN /sys/kernel/config/usb_gadget/g/strings/0x409/manufacturer echo devkit /sys/kernel/config/usb_gadget/g/strings/0x409/product mkdir -p /sys/kernel/config/usb_gadget/g/functions/uac1.usb0 # audio echo 0x1 /sys/kernel/config/usb_gadget/g/functions/uac1.usb0/c_chmask echo 48000 /sys/kernel/config/usb_gadget/g/functions/uac1.usb0/c_srate echo 0xf /sys/kernel/config/usb_gadget/g/functions/uac1.usb0/p_chmask echo 16000 /sys/kernel/config/usb_gadget/g/functions/uac1.usb0/p_srate mkdir -p /sys/kernel/config/usb_gadget/g/configs/c.1 echo 250 /sys/kernel/config/usb_gadget/g/configs/c.1/MaxPower ln -s /sys/kernel/config/usb_gadget/g/functions/uac1.usb0 /sys/kernel/config/usb_gadget/g/configs/c.1/ udevadm settle -t 5 || : ls /sys/class/udc/ /sys/kernel/config/usb_gadget/g/UDC run aplay -l and areocrd -l to check the virtual capture/playback devices created by the script $ aplay -l **** List of PLAYBACK Hardware Devices **** card 0: Codec [H3 Audio Codec], device 0: CDC PCM Codec-0 [] Subdevices: 1/1 Subdevice #0: subdevice #0 card 2: UAC1Gadget [UAC1_Gadget], device 0: UAC1_PCM [UAC1_PCM] Subdevices: 1/1 Subdevice #0: subdevice #0 $ arecord -l **** List of CAPTURE Hardware Devices **** card 0: Codec [H3 Audio Codec], device 0: CDC PCM Codec-0 [] Subdevices: 1/1 Subdevice #0: subdevice #0 card 1: voicen4mic [voicen4mic], device 0: 1c22000.i2s-ac108-codec0 ac108-codec.0-003b-0 [] Subdevices: 1/1 Subdevice #0: subdevice #0 card 2: UAC1Gadget [UAC1_Gadget], device 0: UAC1_PCM [UAC1_PCM] Subdevices: 1/1 Subdevice #0: subdevice #0 when playing an audio through the USB sound card on a computer, the audio is sent to the capture device hw:UAC1Gadget , and then run arecord -f S16_LE -c 1 -r 48000 -D hw:UAC1Gadget | aplay to play the audio on the device. when recording through the sound card on the computer, data is sent from the playback device hw:UAC1Gadget to the computer. a. use Audacity to record audio on a computer. To reocrd 4 (more than 2) channels audio, Windows WASAPI is selected b. run arecord -f S16_LE -c 4 -r 16000 | aplay -D hw:UAC1Gadget to send microphone data to the computer.","title":"As a USB sound card"},{"location":"voicen_os/","text":"VOICEN OS VOICEN OS is based on Armbian and focuses on building voice-enabled devices. It has some built-in apps: Hey WiFi, a web app which can send data over sound to setup WiFi ttyd, a web terminal JupyterLab, a powerful web IDE for Jupyter notebooks Shairport-Sync, an AirPlay audio player Download from OneDrive from \u767e\u5ea6\u7f51\u76d8 Changelog 1.0 2019-11-06 based on Armbian Bionic 4.19.57 with ttyd 1.5.2, JupyterLab 1.1.4","title":"VOICEN OS"},{"location":"voicen_os/#voicen-os","text":"VOICEN OS is based on Armbian and focuses on building voice-enabled devices. It has some built-in apps: Hey WiFi, a web app which can send data over sound to setup WiFi ttyd, a web terminal JupyterLab, a powerful web IDE for Jupyter notebooks Shairport-Sync, an AirPlay audio player","title":"VOICEN OS"},{"location":"voicen_os/#download","text":"from OneDrive from \u767e\u5ea6\u7f51\u76d8","title":"Download"},{"location":"voicen_os/#changelog","text":"1.0 2019-11-06 based on Armbian Bionic 4.19.57 with ttyd 1.5.2, JupyterLab 1.1.4","title":"Changelog"},{"location":"audio_processing/aec/","text":"Acoustic Echo Cancellation In a smart speaker, the algorithm Acoustic Echo Cancellation (AEC) is used to cancel music, which is played by itself, from the audio captured by its microphones, so it can hear your voice clearly when it is playing music. The open source library speexdsp has a AEC algorithm. There are two examples to use it in Python and C. using AEC in Python pip3 install speexdsp create a python script named ec.py Acoustic Echo Cancellation for wav files. import wave import sys from speexdsp import EchoCanceller if len(sys.argv) 4: print( Usage: {} near.wav far.wav out.wav .format(sys.argv[0])) sys.exit(1) frame_size = 256 near = wave.open(sys.argv[1], rb ) far = wave.open(sys.argv[2], rb ) if near.getnchannels() 1 or far.getnchannels() 1: print( Only support mono channel ) sys.exit(2) out = wave.open(sys.argv[3], wb ) out.setnchannels(near.getnchannels()) out.setsampwidth(near.getsampwidth()) out.setframerate(near.getframerate()) print( near - rate: {}, channels: {}, length: {} .format( near.getframerate(), near.getnchannels(), near.getnframes() / near.getframerate())) print( far - rate: {}, channels: {} .format(far.getframerate(), far.getnchannels())) echo_canceller = EchoCanceller.create(frame_size, 2048, near.getframerate()) in_data_len = frame_size in_data_bytes = frame_size * 2 out_data_len = frame_size out_data_bytes = frame_size * 2 while True: in_data = near.readframes(in_data_len) out_data = far.readframes(out_data_len) if len(in_data) != in_data_bytes or len(out_data) != out_data_bytes: break in_data = echo_canceller.process(in_data, out_data) out.writeframes(in_data) near.close() far.close() out.close() play a music (for example, music.wav ) and record it as rec.wav , and run python ec.py rec.wav music.wav out.wav . Note: only mono music is supported. To get a good result, music.wav and rec.wav should be aligned. using AEC in C See ec","title":"AEC"},{"location":"audio_processing/aec/#acoustic-echo-cancellation","text":"In a smart speaker, the algorithm Acoustic Echo Cancellation (AEC) is used to cancel music, which is played by itself, from the audio captured by its microphones, so it can hear your voice clearly when it is playing music. The open source library speexdsp has a AEC algorithm. There are two examples to use it in Python and C.","title":"Acoustic Echo Cancellation"},{"location":"audio_processing/aec/#using-aec-in-python","text":"pip3 install speexdsp create a python script named ec.py Acoustic Echo Cancellation for wav files. import wave import sys from speexdsp import EchoCanceller if len(sys.argv) 4: print( Usage: {} near.wav far.wav out.wav .format(sys.argv[0])) sys.exit(1) frame_size = 256 near = wave.open(sys.argv[1], rb ) far = wave.open(sys.argv[2], rb ) if near.getnchannels() 1 or far.getnchannels() 1: print( Only support mono channel ) sys.exit(2) out = wave.open(sys.argv[3], wb ) out.setnchannels(near.getnchannels()) out.setsampwidth(near.getsampwidth()) out.setframerate(near.getframerate()) print( near - rate: {}, channels: {}, length: {} .format( near.getframerate(), near.getnchannels(), near.getnframes() / near.getframerate())) print( far - rate: {}, channels: {} .format(far.getframerate(), far.getnchannels())) echo_canceller = EchoCanceller.create(frame_size, 2048, near.getframerate()) in_data_len = frame_size in_data_bytes = frame_size * 2 out_data_len = frame_size out_data_bytes = frame_size * 2 while True: in_data = near.readframes(in_data_len) out_data = far.readframes(out_data_len) if len(in_data) != in_data_bytes or len(out_data) != out_data_bytes: break in_data = echo_canceller.process(in_data, out_data) out.writeframes(in_data) near.close() far.close() out.close() play a music (for example, music.wav ) and record it as rec.wav , and run python ec.py rec.wav music.wav out.wav . Note: only mono music is supported. To get a good result, music.wav and rec.wav should be aligned.","title":"using AEC in Python"},{"location":"audio_processing/aec/#using-aec-in-c","text":"See ec","title":"using AEC in C"},{"location":"audio_processing/beamforming/","text":"Beamforming Beamforming is a signal processing technique used in mic arrays to enhance or reduce directional signal. Delay Sum, MVDR, GSC","title":"Beamforming"},{"location":"audio_processing/beamforming/#beamforming","text":"Beamforming is a signal processing technique used in mic arrays to enhance or reduce directional signal. Delay Sum, MVDR, GSC","title":"Beamforming"},{"location":"audio_processing/doa/","text":"DOA Direction Of Arrival (DOA) is used to find the direction of a sound source. ODAS , a library dedicated to perform sound source localization, tracking, separation and post-filtering GCC-PHAT and SRP-PHAT in Matlab, C and C++ GCC-PHAT in Python and Octave","title":"DOA"},{"location":"audio_processing/doa/#doa","text":"Direction Of Arrival (DOA) is used to find the direction of a sound source. ODAS , a library dedicated to perform sound source localization, tracking, separation and post-filtering GCC-PHAT and SRP-PHAT in Matlab, C and C++ GCC-PHAT in Python and Octave","title":"DOA"},{"location":"audio_processing/ns/","text":"Noise Suppression Noise Suppression (NS) is widely used in audio systems. In a smart speaker, NS is requred to remove noise without introducing too much audio distortion which is farmful to speech recognition. Among a variety of NS algorithms, the NS in WebRTC's audio processing module is a robust one. We are going to learn how to use it. NS of WebRTC Audio Processing in Python python-webrtc-audio-processing is used here. sudo pip3 install webrtc-audio-processing create a python script named ns.py import sys import wave from webrtc_audio_processing import AudioProcessingModule as AP if len(sys.argv) 3: print( Usage: {} noisy.wav out.wav .format(sys.argv[0])) sys.exit(1) frame_size = 256 noisy = wave.open(sys.argv[1], rb ) out = wave.open(sys.argv[2], wb ) channels = noisy.getnchannels() sample_width = noisy.getsampwidth() # only 16 bits is supported rate = noisy.getframerate() out.setnchannels(channels) out.setsampwidth(sample_width) out.setframerate(rate) ap = AP(enable_vad=True, enable_ns=True) ap.set_stream_format(rate, channels) # set sample rate and channels ap.set_ns_level(1) # NS level from 0 to 3 ap.set_vad_level(1) # VAD level from 0 to 3 frames_10ms = int(rate / 100) while True: in_data = near.readframes(frames_10ms) if len(in_data) != (frames_10ms * channels * sample_width): break # only support processing 10ms audio data each time audio_out = ap.process_stream(in_data) out.writeframes(in_data) noisy.close() out.close() python3 ns.py noisy.wav out.wav","title":"NS"},{"location":"audio_processing/ns/#noise-suppression","text":"Noise Suppression (NS) is widely used in audio systems. In a smart speaker, NS is requred to remove noise without introducing too much audio distortion which is farmful to speech recognition. Among a variety of NS algorithms, the NS in WebRTC's audio processing module is a robust one. We are going to learn how to use it.","title":"Noise Suppression"},{"location":"audio_processing/ns/#ns-of-webrtc-audio-processing-in-python","text":"python-webrtc-audio-processing is used here. sudo pip3 install webrtc-audio-processing create a python script named ns.py import sys import wave from webrtc_audio_processing import AudioProcessingModule as AP if len(sys.argv) 3: print( Usage: {} noisy.wav out.wav .format(sys.argv[0])) sys.exit(1) frame_size = 256 noisy = wave.open(sys.argv[1], rb ) out = wave.open(sys.argv[2], wb ) channels = noisy.getnchannels() sample_width = noisy.getsampwidth() # only 16 bits is supported rate = noisy.getframerate() out.setnchannels(channels) out.setsampwidth(sample_width) out.setframerate(rate) ap = AP(enable_vad=True, enable_ns=True) ap.set_stream_format(rate, channels) # set sample rate and channels ap.set_ns_level(1) # NS level from 0 to 3 ap.set_vad_level(1) # VAD level from 0 to 3 frames_10ms = int(rate / 100) while True: in_data = near.readframes(frames_10ms) if len(in_data) != (frames_10ms * channels * sample_width): break # only support processing 10ms audio data each time audio_out = ap.process_stream(in_data) out.writeframes(in_data) noisy.close() out.close() python3 ns.py noisy.wav out.wav","title":"NS of WebRTC Audio Processing in Python"}]}